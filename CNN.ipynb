{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using a CNN to classify DSF data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import all the modules/classes/functions\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Conv1D, MaxPool1D, Softmax, Dropout, Input, Flatten, BatchNormalization, Reshape\n",
    "from keras.layers import LeakyReLU\n",
    "from keras import optimizers\n",
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "from keras.wrappers.scikit_learn import KerasClassifier, KerasRegressor\n",
    "from keras.utils import to_categorical\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3700, 100, 1) (3700, 4)\n"
     ]
    }
   ],
   "source": [
    "# import data and labels from stored archive and encode labels\n",
    "def import_data(filename, num_classes=4):\n",
    "    \"\"\"imports the data/labels from a npz file and stores it into two np arrays,\n",
    "    encodes the labels using the one-hot-encoding with the num_classes param\"\"\"\n",
    "    with np.load(filename) as npzfile:\n",
    "        data = npzfile[\"data\"]\n",
    "        labels = npzfile[\"labels\"]\n",
    "    # convert labels to one-hot-encoding\n",
    "    labels_enc = to_categorical(labels, num_classes=num_classes)\n",
    "    return data, labels_enc\n",
    "\n",
    "#load augmented non-scaled data\n",
    "data_ns_aug, labels_ns_aug = import_data(\"data_targets_aug.npz\")\n",
    "\n",
    "# shuffle the data/labels arrays consistently (only for augmented data)\n",
    "data_ns_aug, labels_ns_aug = shuffle(data_ns_aug, labels_ns_aug, random_state=123)\n",
    "\n",
    "# reshape data into a rank 3 tensor: (n_samples, 100, 1). Required by 1D conv layer\n",
    "data_ns_aug = data_ns_aug.reshape(data_ns_aug.shape[0], data_ns_aug.shape[1], 1)\n",
    "#display(data)\n",
    "#display(labels)\n",
    "print(data_ns_aug.shape, labels_ns_aug.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ideas and things to try\n",
    "\n",
    "## Types of NNs:\n",
    "\n",
    "1. CNNs\n",
    "2. RNNs (LSTM)\n",
    "\n",
    "## CNN architectures/params to try:\n",
    "\n",
    "- 2x 1D convolutions (#filters=16, filter_size=3, activation=ReLU)\n",
    "- Batch normalization (?) (maybe I add it after each conv)\n",
    "- Max pooling after each conv step (size=2)\n",
    "- Flatten tensor prior to 1st FC layer\n",
    "- 1 FC hidden layer (32 units to start with, activation=ReLU)\n",
    "- 1 output layer (4 units, activation=Softmax)\n",
    "- If overfitting, add dropout layers! (p=0.5)\n",
    "\n",
    "### backprop params:\n",
    "\n",
    "- loss func: categorical cross-entropy\n",
    "- optm. algorithm: adam (or sgd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# construct a test network\n",
    "\n",
    "class NeuralTester(object):\n",
    "    \"\"\"class for testing neural net architectures\"\"\"\n",
    "    def __init__(self, base_build_fn, data, labels, epochs=50, batch_size=32, keras_api='sequential'):\n",
    "        \"\"\"base_build_fn - a function object that builds the basic network (returns a keras model)\n",
    "        data - rank 2 tensor of shape (n_samples, 100)\n",
    "        labels - binarized labels, rank 2 tensor of shape (n_samples, 4)\"\"\"\n",
    "        self.base_build_fn = base_build_fn\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.keras_api = keras_api\n",
    "        \n",
    "    def fit_predict(self, build_fn, scikit_wrapper, target_name):\n",
    "        \"\"\"Create scikit-estimator from a keras build function, fit to training, predict on test\"\"\"\n",
    "                \n",
    "        # Information for user\n",
    "        print(\"\\nFitting the scikit estimator {}\\n{}\".format(scikit_wrapper.__name__, '='*40))\n",
    "        build_fn().summary() # create a keras model (build the graph)\n",
    "        \n",
    "        # Create the sklearn estimator (either KerasClassifier or KerasRegressor)\n",
    "        estimator = scikit_wrapper(\n",
    "            build_fn=build_fn, \n",
    "            epochs=self.epochs, \n",
    "            batch_size=self.batch_size, \n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        # split data to train/validation sets\n",
    "        data_train, data_test, labels_train, labels_test = train_test_split(self.data, self.labels,\n",
    "                                                                           test_size=0.3,\n",
    "                                                                           random_state=0,\n",
    "                                                                           stratify=self.labels)\n",
    "        \n",
    "        # Fit the estimator using the training set\n",
    "        estimator.fit(data_train, labels_train, \n",
    "            validation_data=(data_test, labels_test), \n",
    "            callbacks=[\n",
    "                ReduceLROnPlateau(patience=3, verbose=1, factor=0.1),\n",
    "                EarlyStopping(patience=5, verbose=1)\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        # Return predictions on test set\n",
    "        return estimator.predict(data_test)\n",
    "    \n",
    "    def create_build_fn(self, loss, activation, optimizer='adam', metrics=[]):\n",
    "        \"\"\"Creates a final build function, depending on the task at hand and the model API used\"\"\"\n",
    "        if self.keras_api == 'sequential':\n",
    "            def build_fn():\n",
    "                model = self.base_build_fn()\n",
    "                model.add(Dense(4, activation=activation)) # 4 classes\n",
    "                model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
    "                return model\n",
    "            return build_fn\n",
    "        else:\n",
    "            def build_fn():\n",
    "                inputs, outputs = self.base_build_fn()\n",
    "                outputs = Dense(1, activation=activation)(outputs)\n",
    "                model = Model(inputs=inputs, outputs=outputs)\n",
    "                model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
    "                return model\n",
    "            return build_fn\n",
    "        \n",
    "    def run_regression(self):\n",
    "        \"\"\"Use the build function to attempt regression task\"\"\"\n",
    "        return self.fit_predict(\n",
    "            self.create_build_fn('mean_squared_error', 'linear'), \n",
    "            KerasRegressor, \n",
    "            'y_regr'\n",
    "        )\n",
    "        \n",
    "    def run_classification(self):\n",
    "        \"\"\"Use the build function to attempt classification task\"\"\"\n",
    "        return self.fit_predict(\n",
    "            self.create_build_fn('categorical_crossentropy', 'softmax', metrics=['accuracy']), \n",
    "            KerasClassifier, \n",
    "            'y_cls'\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a simple FC model and test its performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fitting the scikit estimator KerasClassifier\n",
      "========================================\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_13 (Flatten)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 16)                1616      \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 4)                 68        \n",
      "=================================================================\n",
      "Total params: 1,684\n",
      "Trainable params: 1,684\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 2590 samples, validate on 1110 samples\n",
      "Epoch 1/100\n",
      "2590/2590 [==============================] - 1s 238us/step - loss: 1.4613 - acc: 0.3220 - val_loss: 1.3131 - val_acc: 0.3883\n",
      "Epoch 2/100\n",
      "2590/2590 [==============================] - 0s 15us/step - loss: 1.3665 - acc: 0.3969 - val_loss: 1.3043 - val_acc: 0.4045\n",
      "Epoch 3/100\n",
      "2590/2590 [==============================] - 0s 14us/step - loss: 1.3599 - acc: 0.3981 - val_loss: 1.2964 - val_acc: 0.4045\n",
      "Epoch 4/100\n",
      "2590/2590 [==============================] - 0s 14us/step - loss: 1.3541 - acc: 0.4015 - val_loss: 1.2928 - val_acc: 0.3955\n",
      "Epoch 5/100\n",
      "2590/2590 [==============================] - 0s 14us/step - loss: 1.3506 - acc: 0.4012 - val_loss: 1.2891 - val_acc: 0.4036\n",
      "Epoch 6/100\n",
      "2590/2590 [==============================] - 0s 14us/step - loss: 1.3465 - acc: 0.3992 - val_loss: 1.2827 - val_acc: 0.3577\n",
      "Epoch 7/100\n",
      "2590/2590 [==============================] - 0s 14us/step - loss: 1.2437 - acc: 0.3707 - val_loss: 1.2237 - val_acc: 0.4000\n",
      "Epoch 8/100\n",
      "2590/2590 [==============================] - 0s 14us/step - loss: 1.2183 - acc: 0.3954 - val_loss: 1.2222 - val_acc: 0.3604\n",
      "Epoch 9/100\n",
      "2590/2590 [==============================] - 0s 14us/step - loss: 1.2182 - acc: 0.3865 - val_loss: 1.2190 - val_acc: 0.3955\n",
      "Epoch 10/100\n",
      "2590/2590 [==============================] - 0s 14us/step - loss: 1.2143 - acc: 0.4073 - val_loss: 1.2206 - val_acc: 0.3613\n",
      "Epoch 11/100\n",
      "2590/2590 [==============================] - 0s 14us/step - loss: 1.2141 - acc: 0.3880 - val_loss: 1.2160 - val_acc: 0.3883\n",
      "Epoch 12/100\n",
      "2590/2590 [==============================] - 0s 14us/step - loss: 1.2117 - acc: 0.3892 - val_loss: 1.2151 - val_acc: 0.3577\n",
      "Epoch 13/100\n",
      "2590/2590 [==============================] - 0s 14us/step - loss: 1.2100 - acc: 0.3853 - val_loss: 1.2140 - val_acc: 0.3820\n",
      "Epoch 14/100\n",
      "2590/2590 [==============================] - 0s 14us/step - loss: 1.2084 - acc: 0.3900 - val_loss: 1.2117 - val_acc: 0.3892\n",
      "Epoch 15/100\n",
      "2590/2590 [==============================] - 0s 14us/step - loss: 1.2074 - acc: 0.3903 - val_loss: 1.2136 - val_acc: 0.4180\n",
      "Epoch 16/100\n",
      "2590/2590 [==============================] - 0s 14us/step - loss: 1.2053 - acc: 0.3903 - val_loss: 1.2111 - val_acc: 0.3937\n",
      "Epoch 17/100\n",
      "2590/2590 [==============================] - 0s 14us/step - loss: 1.2037 - acc: 0.4019 - val_loss: 1.2073 - val_acc: 0.3829\n",
      "Epoch 18/100\n",
      "2590/2590 [==============================] - 0s 15us/step - loss: 1.2016 - acc: 0.3954 - val_loss: 1.2042 - val_acc: 0.3649\n",
      "Epoch 19/100\n",
      "2590/2590 [==============================] - 0s 14us/step - loss: 1.2010 - acc: 0.3946 - val_loss: 1.2038 - val_acc: 0.3604\n",
      "Epoch 20/100\n",
      "2590/2590 [==============================] - 0s 14us/step - loss: 1.1985 - acc: 0.3838 - val_loss: 1.2023 - val_acc: 0.3541\n",
      "Epoch 21/100\n",
      "2590/2590 [==============================] - 0s 14us/step - loss: 1.1957 - acc: 0.3788 - val_loss: 1.2020 - val_acc: 0.3766\n",
      "Epoch 22/100\n",
      "2590/2590 [==============================] - 0s 14us/step - loss: 1.1956 - acc: 0.3931 - val_loss: 1.2044 - val_acc: 0.4396\n",
      "Epoch 23/100\n",
      "2590/2590 [==============================] - 0s 14us/step - loss: 1.1955 - acc: 0.3834 - val_loss: 1.1971 - val_acc: 0.3568\n",
      "Epoch 24/100\n",
      "2590/2590 [==============================] - 0s 14us/step - loss: 1.1920 - acc: 0.3900 - val_loss: 1.1985 - val_acc: 0.3748\n",
      "Epoch 25/100\n",
      "2590/2590 [==============================] - 0s 14us/step - loss: 1.1910 - acc: 0.3907 - val_loss: 1.1942 - val_acc: 0.3586\n",
      "Epoch 26/100\n",
      "2590/2590 [==============================] - 0s 14us/step - loss: 1.1887 - acc: 0.3923 - val_loss: 1.1957 - val_acc: 0.4568\n",
      "Epoch 27/100\n",
      "2590/2590 [==============================] - 0s 14us/step - loss: 1.1864 - acc: 0.4116 - val_loss: 1.1916 - val_acc: 0.3982\n",
      "Epoch 28/100\n",
      "2590/2590 [==============================] - 0s 14us/step - loss: 1.1839 - acc: 0.4069 - val_loss: 1.1982 - val_acc: 0.3865\n",
      "Epoch 29/100\n",
      "2590/2590 [==============================] - 0s 15us/step - loss: 1.1842 - acc: 0.4012 - val_loss: 1.1901 - val_acc: 0.3694\n",
      "Epoch 30/100\n",
      "2590/2590 [==============================] - 0s 14us/step - loss: 1.1815 - acc: 0.4166 - val_loss: 1.1872 - val_acc: 0.3667\n",
      "Epoch 31/100\n",
      "2590/2590 [==============================] - 0s 14us/step - loss: 1.1775 - acc: 0.4008 - val_loss: 1.1843 - val_acc: 0.3613\n",
      "Epoch 32/100\n",
      "2590/2590 [==============================] - 0s 14us/step - loss: 1.1770 - acc: 0.4108 - val_loss: 1.1826 - val_acc: 0.4144\n",
      "Epoch 33/100\n",
      "2590/2590 [==============================] - 0s 14us/step - loss: 1.1758 - acc: 0.3931 - val_loss: 1.1785 - val_acc: 0.3892\n",
      "Epoch 34/100\n",
      "2590/2590 [==============================] - 0s 14us/step - loss: 1.1744 - acc: 0.4278 - val_loss: 1.1809 - val_acc: 0.4369\n",
      "Epoch 35/100\n",
      "2590/2590 [==============================] - 0s 14us/step - loss: 1.1693 - acc: 0.4023 - val_loss: 1.1748 - val_acc: 0.3748\n",
      "Epoch 36/100\n",
      "2590/2590 [==============================] - 0s 14us/step - loss: 1.1692 - acc: 0.4251 - val_loss: 1.1743 - val_acc: 0.4234\n",
      "Epoch 37/100\n",
      "2590/2590 [==============================] - 0s 14us/step - loss: 1.1663 - acc: 0.4112 - val_loss: 1.1727 - val_acc: 0.4108\n",
      "Epoch 38/100\n",
      "2590/2590 [==============================] - 0s 14us/step - loss: 1.1636 - acc: 0.4108 - val_loss: 1.1700 - val_acc: 0.3901\n",
      "Epoch 39/100\n",
      "2590/2590 [==============================] - 0s 15us/step - loss: 1.1607 - acc: 0.4197 - val_loss: 1.1676 - val_acc: 0.3721\n",
      "Epoch 40/100\n",
      "2590/2590 [==============================] - 0s 15us/step - loss: 1.1604 - acc: 0.4185 - val_loss: 1.1664 - val_acc: 0.3640\n",
      "Epoch 41/100\n",
      "2590/2590 [==============================] - 0s 14us/step - loss: 1.1578 - acc: 0.4151 - val_loss: 1.1622 - val_acc: 0.4171\n",
      "Epoch 42/100\n",
      "2590/2590 [==============================] - 0s 14us/step - loss: 1.1544 - acc: 0.4251 - val_loss: 1.1609 - val_acc: 0.3757\n",
      "Epoch 43/100\n",
      "2590/2590 [==============================] - 0s 14us/step - loss: 1.1520 - acc: 0.4270 - val_loss: 1.1577 - val_acc: 0.4045\n",
      "Epoch 44/100\n",
      "2590/2590 [==============================] - 0s 14us/step - loss: 1.1504 - acc: 0.4471 - val_loss: 1.1595 - val_acc: 0.3739\n",
      "Epoch 45/100\n",
      "2590/2590 [==============================] - 0s 14us/step - loss: 1.1490 - acc: 0.4154 - val_loss: 1.1535 - val_acc: 0.4054\n",
      "Epoch 46/100\n",
      "2590/2590 [==============================] - 0s 14us/step - loss: 1.1474 - acc: 0.4290 - val_loss: 1.1541 - val_acc: 0.4667\n",
      "Epoch 47/100\n",
      "2590/2590 [==============================] - 0s 14us/step - loss: 1.1428 - acc: 0.4506 - val_loss: 1.1569 - val_acc: 0.3775\n",
      "Epoch 48/100\n",
      "2590/2590 [==============================] - 0s 15us/step - loss: 1.1401 - acc: 0.4340 - val_loss: 1.1523 - val_acc: 0.5189\n",
      "Epoch 49/100\n",
      "2590/2590 [==============================] - 0s 15us/step - loss: 1.1392 - acc: 0.4568 - val_loss: 1.1454 - val_acc: 0.4180\n",
      "Epoch 50/100\n",
      "2590/2590 [==============================] - 0s 15us/step - loss: 1.1364 - acc: 0.4355 - val_loss: 1.1450 - val_acc: 0.3919\n",
      "Epoch 51/100\n",
      "2590/2590 [==============================] - 0s 15us/step - loss: 1.1363 - acc: 0.4386 - val_loss: 1.1472 - val_acc: 0.4910\n",
      "Epoch 52/100\n",
      "2590/2590 [==============================] - 0s 14us/step - loss: 1.1333 - acc: 0.4598 - val_loss: 1.1395 - val_acc: 0.4297\n",
      "Epoch 53/100\n",
      "2590/2590 [==============================] - 0s 14us/step - loss: 1.1301 - acc: 0.4506 - val_loss: 1.1368 - val_acc: 0.4189\n",
      "Epoch 54/100\n",
      "2590/2590 [==============================] - 0s 14us/step - loss: 1.1298 - acc: 0.4444 - val_loss: 1.1380 - val_acc: 0.4180\n",
      "Epoch 55/100\n",
      "2590/2590 [==============================] - 0s 14us/step - loss: 1.1263 - acc: 0.4541 - val_loss: 1.1374 - val_acc: 0.3910\n",
      "Epoch 56/100\n",
      "2590/2590 [==============================] - 0s 14us/step - loss: 1.1317 - acc: 0.4695 - val_loss: 1.1380 - val_acc: 0.5144\n",
      "Epoch 57/100\n",
      "2590/2590 [==============================] - 0s 14us/step - loss: 1.1236 - acc: 0.4490 - val_loss: 1.1323 - val_acc: 0.5153\n",
      "Epoch 58/100\n",
      "2590/2590 [==============================] - 0s 14us/step - loss: 1.1239 - acc: 0.4614 - val_loss: 1.1366 - val_acc: 0.4198\n",
      "Epoch 59/100\n",
      "2590/2590 [==============================] - 0s 14us/step - loss: 1.1250 - acc: 0.4637 - val_loss: 1.1287 - val_acc: 0.5045\n",
      "Epoch 60/100\n",
      "2590/2590 [==============================] - 0s 14us/step - loss: 1.1196 - acc: 0.4575 - val_loss: 1.1249 - val_acc: 0.5036\n",
      "Epoch 61/100\n",
      "2590/2590 [==============================] - 0s 14us/step - loss: 1.1164 - acc: 0.4618 - val_loss: 1.1241 - val_acc: 0.5018\n",
      "Epoch 62/100\n",
      "2590/2590 [==============================] - 0s 14us/step - loss: 1.1150 - acc: 0.4668 - val_loss: 1.1223 - val_acc: 0.4919\n",
      "Epoch 63/100\n",
      "2590/2590 [==============================] - 0s 14us/step - loss: 1.1115 - acc: 0.4768 - val_loss: 1.1206 - val_acc: 0.4577\n",
      "Epoch 64/100\n",
      "2590/2590 [==============================] - 0s 14us/step - loss: 1.1095 - acc: 0.4934 - val_loss: 1.1180 - val_acc: 0.5018\n",
      "Epoch 65/100\n",
      "2590/2590 [==============================] - 0s 14us/step - loss: 1.1082 - acc: 0.4618 - val_loss: 1.1224 - val_acc: 0.4216\n",
      "Epoch 66/100\n",
      "2590/2590 [==============================] - 0s 14us/step - loss: 1.1140 - acc: 0.4529 - val_loss: 1.1340 - val_acc: 0.4027\n",
      "Epoch 67/100\n",
      "2590/2590 [==============================] - 0s 14us/step - loss: 1.1062 - acc: 0.5000 - val_loss: 1.1147 - val_acc: 0.5171\n",
      "Epoch 68/100\n",
      "2590/2590 [==============================] - 0s 14us/step - loss: 1.1050 - acc: 0.4888 - val_loss: 1.1120 - val_acc: 0.5036\n",
      "Epoch 69/100\n",
      "2590/2590 [==============================] - 0s 14us/step - loss: 1.1042 - acc: 0.4737 - val_loss: 1.1102 - val_acc: 0.5027\n",
      "Epoch 70/100\n",
      "2590/2590 [==============================] - 0s 14us/step - loss: 1.0992 - acc: 0.4900 - val_loss: 1.1109 - val_acc: 0.4297\n",
      "Epoch 71/100\n",
      "2590/2590 [==============================] - 0s 15us/step - loss: 1.0997 - acc: 0.4830 - val_loss: 1.1068 - val_acc: 0.4937\n",
      "Epoch 72/100\n",
      "2590/2590 [==============================] - 0s 15us/step - loss: 1.0982 - acc: 0.4865 - val_loss: 1.1116 - val_acc: 0.4766\n",
      "Epoch 73/100\n",
      "2590/2590 [==============================] - 0s 14us/step - loss: 1.0958 - acc: 0.4907 - val_loss: 1.1060 - val_acc: 0.4901\n",
      "Epoch 74/100\n",
      "2590/2590 [==============================] - 0s 14us/step - loss: 1.0944 - acc: 0.4981 - val_loss: 1.1070 - val_acc: 0.5126\n",
      "Epoch 75/100\n",
      "2590/2590 [==============================] - 0s 14us/step - loss: 1.0943 - acc: 0.4988 - val_loss: 1.1164 - val_acc: 0.3982\n",
      "Epoch 76/100\n",
      "2590/2590 [==============================] - 0s 14us/step - loss: 1.0921 - acc: 0.4869 - val_loss: 1.1015 - val_acc: 0.5207\n",
      "Epoch 77/100\n",
      "2590/2590 [==============================] - 0s 14us/step - loss: 1.0906 - acc: 0.5046 - val_loss: 1.0990 - val_acc: 0.4946\n",
      "Epoch 78/100\n",
      "2590/2590 [==============================] - 0s 14us/step - loss: 1.0894 - acc: 0.4961 - val_loss: 1.0990 - val_acc: 0.5234\n",
      "Epoch 79/100\n",
      "2590/2590 [==============================] - 0s 14us/step - loss: 1.0886 - acc: 0.5066 - val_loss: 1.0961 - val_acc: 0.5072\n",
      "Epoch 80/100\n",
      "2590/2590 [==============================] - 0s 15us/step - loss: 1.0891 - acc: 0.4981 - val_loss: 1.0946 - val_acc: 0.5072\n",
      "Epoch 81/100\n",
      "2590/2590 [==============================] - 0s 14us/step - loss: 1.0854 - acc: 0.5031 - val_loss: 1.0951 - val_acc: 0.5261\n",
      "Epoch 82/100\n",
      "2590/2590 [==============================] - 0s 14us/step - loss: 1.0938 - acc: 0.4780 - val_loss: 1.1334 - val_acc: 0.4676\n",
      "Epoch 83/100\n",
      "2590/2590 [==============================] - 0s 14us/step - loss: 1.0895 - acc: 0.5081 - val_loss: 1.0958 - val_acc: 0.4613\n",
      "Epoch 84/100\n",
      "2590/2590 [==============================] - 0s 14us/step - loss: 1.0836 - acc: 0.4950 - val_loss: 1.0902 - val_acc: 0.5207\n",
      "Epoch 85/100\n",
      "2590/2590 [==============================] - 0s 14us/step - loss: 1.0798 - acc: 0.5127 - val_loss: 1.0894 - val_acc: 0.5252\n",
      "Epoch 86/100\n",
      "2590/2590 [==============================] - 0s 14us/step - loss: 1.0802 - acc: 0.5131 - val_loss: 1.0881 - val_acc: 0.5207\n",
      "Epoch 87/100\n",
      "2590/2590 [==============================] - 0s 14us/step - loss: 1.0770 - acc: 0.5166 - val_loss: 1.0901 - val_acc: 0.5072\n",
      "Epoch 88/100\n",
      "2590/2590 [==============================] - 0s 14us/step - loss: 1.0776 - acc: 0.5089 - val_loss: 1.0871 - val_acc: 0.5207\n",
      "Epoch 89/100\n",
      "2590/2590 [==============================] - 0s 14us/step - loss: 1.0773 - acc: 0.5104 - val_loss: 1.0851 - val_acc: 0.5225\n",
      "Epoch 90/100\n",
      "2590/2590 [==============================] - 0s 14us/step - loss: 1.0748 - acc: 0.5174 - val_loss: 1.0881 - val_acc: 0.5054\n",
      "Epoch 91/100\n",
      "2590/2590 [==============================] - 0s 14us/step - loss: 1.0748 - acc: 0.5124 - val_loss: 1.0838 - val_acc: 0.5279\n",
      "Epoch 92/100\n",
      "2590/2590 [==============================] - 0s 15us/step - loss: 1.0728 - acc: 0.5151 - val_loss: 1.0826 - val_acc: 0.5126\n",
      "Epoch 93/100\n",
      "2590/2590 [==============================] - 0s 16us/step - loss: 1.0714 - acc: 0.5208 - val_loss: 1.0840 - val_acc: 0.5270\n",
      "Epoch 94/100\n",
      "2590/2590 [==============================] - 0s 14us/step - loss: 1.0720 - acc: 0.5162 - val_loss: 1.0805 - val_acc: 0.5216\n",
      "Epoch 95/100\n",
      "2590/2590 [==============================] - 0s 15us/step - loss: 1.0728 - acc: 0.5135 - val_loss: 1.0807 - val_acc: 0.5279\n",
      "Epoch 96/100\n",
      "2590/2590 [==============================] - 0s 14us/step - loss: 1.0743 - acc: 0.5143 - val_loss: 1.0859 - val_acc: 0.5198\n",
      "Epoch 97/100\n",
      "2590/2590 [==============================] - 0s 14us/step - loss: 1.0700 - acc: 0.5224 - val_loss: 1.0854 - val_acc: 0.4874\n",
      "Epoch 98/100\n",
      "2590/2590 [==============================] - 0s 14us/step - loss: 1.0710 - acc: 0.5081 - val_loss: 1.0769 - val_acc: 0.5216\n",
      "Epoch 99/100\n",
      "2590/2590 [==============================] - 0s 14us/step - loss: 1.0676 - acc: 0.5212 - val_loss: 1.0763 - val_acc: 0.5252\n",
      "Epoch 100/100\n",
      "2590/2590 [==============================] - 0s 14us/step - loss: 1.0657 - acc: 0.5232 - val_loss: 1.0824 - val_acc: 0.4946\n",
      "1110/1110 [==============================] - 0s 196us/step\n"
     ]
    }
   ],
   "source": [
    "def create_model_FC():\n",
    "    \"\"\"creates a simple MLP\"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(Flatten(input_shape=(100,1)))\n",
    "    model.add(Dense(16, activation=\"relu\"))\n",
    "    return model\n",
    "\n",
    "tester_fc = NeuralTester(create_model_FC, data_ns_aug, labels_ns_aug, epochs=100, batch_size=100)\n",
    "clss_fc = tester_fc.run_classification()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a 1D conv model and test its performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fitting the scikit estimator KerasClassifier\n",
      "========================================\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_57 (Conv1D)           (None, 47, 4)             32        \n",
      "_________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling (None, 23, 4)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_58 (Conv1D)           (None, 19, 4)             84        \n",
      "_________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling (None, 9, 4)              0         \n",
      "_________________________________________________________________\n",
      "flatten_31 (Flatten)         (None, 36)                0         \n",
      "_________________________________________________________________\n",
      "dense_61 (Dense)             (None, 64)                2368      \n",
      "_________________________________________________________________\n",
      "dense_62 (Dense)             (None, 4)                 260       \n",
      "=================================================================\n",
      "Total params: 2,744\n",
      "Trainable params: 2,744\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 2590 samples, validate on 1110 samples\n",
      "Epoch 1/150\n",
      "2590/2590 [==============================] - 3s 1ms/step - loss: 1.3002 - acc: 0.4093 - val_loss: 1.2462 - val_acc: 0.4072\n",
      "Epoch 2/150\n",
      "2590/2590 [==============================] - 1s 207us/step - loss: 1.2286 - acc: 0.4081 - val_loss: 1.2343 - val_acc: 0.4072\n",
      "Epoch 3/150\n",
      "2590/2590 [==============================] - 0s 61us/step - loss: 1.2206 - acc: 0.4124 - val_loss: 1.2277 - val_acc: 0.4072\n",
      "Epoch 4/150\n",
      "2590/2590 [==============================] - 0s 86us/step - loss: 1.2178 - acc: 0.4124 - val_loss: 1.2237 - val_acc: 0.4072\n",
      "Epoch 5/150\n",
      "2590/2590 [==============================] - 0s 52us/step - loss: 1.2101 - acc: 0.4124 - val_loss: 1.2151 - val_acc: 0.4072\n",
      "Epoch 6/150\n",
      "2590/2590 [==============================] - 0s 56us/step - loss: 1.2004 - acc: 0.4178 - val_loss: 1.2057 - val_acc: 0.4306\n",
      "Epoch 7/150\n",
      "2590/2590 [==============================] - 0s 75us/step - loss: 1.1871 - acc: 0.4324 - val_loss: 1.1921 - val_acc: 0.4604\n",
      "Epoch 8/150\n",
      "2590/2590 [==============================] - 0s 57us/step - loss: 1.1735 - acc: 0.4637 - val_loss: 1.1785 - val_acc: 0.4550\n",
      "Epoch 9/150\n",
      "2590/2590 [==============================] - 0s 60us/step - loss: 1.1554 - acc: 0.4668 - val_loss: 1.1633 - val_acc: 0.4658\n",
      "Epoch 10/150\n",
      "2590/2590 [==============================] - 0s 60us/step - loss: 1.1409 - acc: 0.4649 - val_loss: 1.1468 - val_acc: 0.4613\n",
      "Epoch 11/150\n",
      "2590/2590 [==============================] - 0s 56us/step - loss: 1.1270 - acc: 0.4784 - val_loss: 1.1366 - val_acc: 0.4829\n",
      "Epoch 12/150\n",
      "2590/2590 [==============================] - 0s 75us/step - loss: 1.1149 - acc: 0.4846 - val_loss: 1.1246 - val_acc: 0.4613\n",
      "Epoch 13/150\n",
      "2590/2590 [==============================] - 0s 54us/step - loss: 1.1030 - acc: 0.4772 - val_loss: 1.1180 - val_acc: 0.4748\n",
      "Epoch 14/150\n",
      "2590/2590 [==============================] - 0s 112us/step - loss: 1.1004 - acc: 0.4873 - val_loss: 1.1084 - val_acc: 0.4658\n",
      "Epoch 15/150\n",
      "2590/2590 [==============================] - 0s 55us/step - loss: 1.0919 - acc: 0.5042 - val_loss: 1.1020 - val_acc: 0.4892\n",
      "Epoch 16/150\n",
      "2590/2590 [==============================] - 0s 54us/step - loss: 1.0848 - acc: 0.5104 - val_loss: 1.1015 - val_acc: 0.5459\n",
      "Epoch 17/150\n",
      "2590/2590 [==============================] - 0s 53us/step - loss: 1.0813 - acc: 0.5174 - val_loss: 1.0960 - val_acc: 0.5297\n",
      "Epoch 18/150\n",
      "2590/2590 [==============================] - 0s 87us/step - loss: 1.0750 - acc: 0.5143 - val_loss: 1.0911 - val_acc: 0.5099\n",
      "Epoch 19/150\n",
      "2590/2590 [==============================] - 0s 56us/step - loss: 1.0659 - acc: 0.5224 - val_loss: 1.0799 - val_acc: 0.4964\n",
      "Epoch 20/150\n",
      "2590/2590 [==============================] - 0s 75us/step - loss: 1.0528 - acc: 0.5444 - val_loss: 1.0765 - val_acc: 0.5775\n",
      "Epoch 21/150\n",
      "2590/2590 [==============================] - 0s 57us/step - loss: 1.0623 - acc: 0.5432 - val_loss: 1.0664 - val_acc: 0.5459\n",
      "Epoch 22/150\n",
      "2590/2590 [==============================] - 0s 53us/step - loss: 1.0422 - acc: 0.5552 - val_loss: 1.0604 - val_acc: 0.5450\n",
      "Epoch 23/150\n",
      "2590/2590 [==============================] - 0s 54us/step - loss: 1.0342 - acc: 0.5660 - val_loss: 1.0618 - val_acc: 0.5441\n",
      "Epoch 24/150\n",
      "2590/2590 [==============================] - 0s 59us/step - loss: 1.0322 - acc: 0.5653 - val_loss: 1.0518 - val_acc: 0.5631\n",
      "Epoch 25/150\n",
      "2590/2590 [==============================] - 0s 56us/step - loss: 1.0212 - acc: 0.5799 - val_loss: 1.0412 - val_acc: 0.5523\n",
      "Epoch 26/150\n",
      "2590/2590 [==============================] - 0s 57us/step - loss: 1.0113 - acc: 0.5892 - val_loss: 1.0362 - val_acc: 0.5468\n",
      "Epoch 27/150\n",
      "2590/2590 [==============================] - 0s 54us/step - loss: 1.0138 - acc: 0.5656 - val_loss: 1.0319 - val_acc: 0.5568\n",
      "Epoch 28/150\n",
      "2590/2590 [==============================] - 0s 55us/step - loss: 1.0076 - acc: 0.5768 - val_loss: 1.0282 - val_acc: 0.5405\n",
      "Epoch 29/150\n",
      "2590/2590 [==============================] - 0s 54us/step - loss: 1.0055 - acc: 0.5792 - val_loss: 1.0330 - val_acc: 0.5396\n",
      "Epoch 30/150\n",
      "2590/2590 [==============================] - 0s 53us/step - loss: 0.9969 - acc: 0.5807 - val_loss: 1.0430 - val_acc: 0.5486\n",
      "Epoch 31/150\n",
      "2590/2590 [==============================] - 0s 54us/step - loss: 0.9880 - acc: 0.5869 - val_loss: 1.0132 - val_acc: 0.5856\n",
      "Epoch 32/150\n",
      "2590/2590 [==============================] - 0s 76us/step - loss: 0.9886 - acc: 0.5884 - val_loss: 1.0150 - val_acc: 0.5441\n",
      "Epoch 33/150\n",
      "2590/2590 [==============================] - 0s 93us/step - loss: 0.9760 - acc: 0.5919 - val_loss: 1.0010 - val_acc: 0.5658\n",
      "Epoch 34/150\n",
      "2590/2590 [==============================] - 0s 80us/step - loss: 0.9669 - acc: 0.5927 - val_loss: 0.9891 - val_acc: 0.5595\n",
      "Epoch 35/150\n",
      "2590/2590 [==============================] - 0s 63us/step - loss: 0.9604 - acc: 0.6054 - val_loss: 1.0001 - val_acc: 0.5505\n",
      "Epoch 36/150\n",
      "2590/2590 [==============================] - 0s 73us/step - loss: 0.9616 - acc: 0.6004 - val_loss: 0.9990 - val_acc: 0.5414\n",
      "Epoch 37/150\n",
      "2590/2590 [==============================] - 0s 72us/step - loss: 0.9502 - acc: 0.6066 - val_loss: 0.9834 - val_acc: 0.5784\n",
      "Epoch 38/150\n",
      "2590/2590 [==============================] - 0s 72us/step - loss: 0.9429 - acc: 0.6112 - val_loss: 0.9689 - val_acc: 0.5667\n",
      "Epoch 39/150\n",
      "2590/2590 [==============================] - 0s 71us/step - loss: 0.9507 - acc: 0.6066 - val_loss: 0.9691 - val_acc: 0.5865\n",
      "Epoch 40/150\n",
      "2590/2590 [==============================] - 0s 54us/step - loss: 0.9317 - acc: 0.6170 - val_loss: 0.9675 - val_acc: 0.5856\n",
      "Epoch 41/150\n",
      "2590/2590 [==============================] - 0s 52us/step - loss: 0.9258 - acc: 0.6116 - val_loss: 0.9605 - val_acc: 0.5910\n",
      "Epoch 42/150\n",
      "2590/2590 [==============================] - 0s 55us/step - loss: 0.9455 - acc: 0.6012 - val_loss: 0.9552 - val_acc: 0.5883\n",
      "Epoch 43/150\n",
      "2590/2590 [==============================] - 0s 53us/step - loss: 0.9228 - acc: 0.6239 - val_loss: 0.9482 - val_acc: 0.6036\n",
      "Epoch 44/150\n",
      "2590/2590 [==============================] - 0s 53us/step - loss: 0.9100 - acc: 0.6220 - val_loss: 0.9462 - val_acc: 0.5946\n",
      "Epoch 45/150\n",
      "2590/2590 [==============================] - 0s 62us/step - loss: 0.9133 - acc: 0.6208 - val_loss: 0.9386 - val_acc: 0.6045\n",
      "Epoch 46/150\n",
      "2590/2590 [==============================] - 0s 62us/step - loss: 0.9037 - acc: 0.6274 - val_loss: 0.9381 - val_acc: 0.6270\n",
      "Epoch 47/150\n",
      "2590/2590 [==============================] - 0s 60us/step - loss: 0.9038 - acc: 0.6251 - val_loss: 0.9383 - val_acc: 0.5856\n",
      "Epoch 48/150\n",
      "2590/2590 [==============================] - 0s 58us/step - loss: 0.8978 - acc: 0.6208 - val_loss: 0.9344 - val_acc: 0.5982\n",
      "Epoch 49/150\n",
      "2590/2590 [==============================] - 0s 58us/step - loss: 0.8970 - acc: 0.6232 - val_loss: 0.9261 - val_acc: 0.6396\n",
      "Epoch 50/150\n",
      "2590/2590 [==============================] - 0s 72us/step - loss: 0.8871 - acc: 0.6405 - val_loss: 0.9249 - val_acc: 0.6297\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51/150\n",
      "2590/2590 [==============================] - 0s 52us/step - loss: 0.8855 - acc: 0.6382 - val_loss: 0.9142 - val_acc: 0.6279\n",
      "Epoch 52/150\n",
      "2590/2590 [==============================] - 0s 53us/step - loss: 0.8792 - acc: 0.6386 - val_loss: 0.9209 - val_acc: 0.6180\n",
      "Epoch 53/150\n",
      "2590/2590 [==============================] - 0s 54us/step - loss: 0.8786 - acc: 0.6409 - val_loss: 0.9141 - val_acc: 0.6495\n",
      "Epoch 54/150\n",
      "2590/2590 [==============================] - 0s 53us/step - loss: 0.8745 - acc: 0.6440 - val_loss: 0.9067 - val_acc: 0.6252\n",
      "Epoch 55/150\n",
      "2590/2590 [==============================] - 0s 54us/step - loss: 0.8744 - acc: 0.6398 - val_loss: 0.9088 - val_acc: 0.6045\n",
      "Epoch 56/150\n",
      "2590/2590 [==============================] - 0s 54us/step - loss: 0.8707 - acc: 0.6452 - val_loss: 0.9055 - val_acc: 0.6261\n",
      "Epoch 57/150\n",
      "2590/2590 [==============================] - 0s 55us/step - loss: 0.8620 - acc: 0.6486 - val_loss: 0.9192 - val_acc: 0.6045\n",
      "Epoch 58/150\n",
      "2590/2590 [==============================] - 0s 59us/step - loss: 0.8589 - acc: 0.6568 - val_loss: 0.8968 - val_acc: 0.6441\n",
      "Epoch 59/150\n",
      "2590/2590 [==============================] - 0s 53us/step - loss: 0.8628 - acc: 0.6541 - val_loss: 0.8937 - val_acc: 0.6396\n",
      "Epoch 60/150\n",
      "2590/2590 [==============================] - 0s 56us/step - loss: 0.8587 - acc: 0.6610 - val_loss: 0.9046 - val_acc: 0.6505\n",
      "Epoch 61/150\n",
      "2590/2590 [==============================] - 0s 75us/step - loss: 0.8541 - acc: 0.6521 - val_loss: 0.8862 - val_acc: 0.6396\n",
      "Epoch 62/150\n",
      "2590/2590 [==============================] - 0s 63us/step - loss: 0.8571 - acc: 0.6475 - val_loss: 0.8824 - val_acc: 0.6532\n",
      "Epoch 63/150\n",
      "2590/2590 [==============================] - 0s 54us/step - loss: 0.8454 - acc: 0.6645 - val_loss: 0.8804 - val_acc: 0.6441\n",
      "Epoch 64/150\n",
      "2590/2590 [==============================] - 0s 51us/step - loss: 0.8420 - acc: 0.6672 - val_loss: 0.9028 - val_acc: 0.6378\n",
      "Epoch 65/150\n",
      "2590/2590 [==============================] - 0s 52us/step - loss: 0.8425 - acc: 0.6552 - val_loss: 0.9034 - val_acc: 0.6162\n",
      "Epoch 66/150\n",
      "2590/2590 [==============================] - 0s 52us/step - loss: 0.8478 - acc: 0.6583 - val_loss: 0.8780 - val_acc: 0.6414\n",
      "Epoch 67/150\n",
      "2590/2590 [==============================] - 0s 53us/step - loss: 0.8326 - acc: 0.6699 - val_loss: 0.8696 - val_acc: 0.6495\n",
      "Epoch 68/150\n",
      "2590/2590 [==============================] - 0s 53us/step - loss: 0.8338 - acc: 0.6641 - val_loss: 0.8861 - val_acc: 0.6351\n",
      "Epoch 69/150\n",
      "2590/2590 [==============================] - 0s 55us/step - loss: 0.8302 - acc: 0.6676 - val_loss: 0.8637 - val_acc: 0.6523\n",
      "Epoch 70/150\n",
      "2590/2590 [==============================] - 0s 56us/step - loss: 0.8242 - acc: 0.6695 - val_loss: 0.8727 - val_acc: 0.6450\n",
      "Epoch 71/150\n",
      "2590/2590 [==============================] - 0s 56us/step - loss: 0.8284 - acc: 0.6625 - val_loss: 0.8620 - val_acc: 0.6523\n",
      "Epoch 72/150\n",
      "2590/2590 [==============================] - 0s 60us/step - loss: 0.8250 - acc: 0.6691 - val_loss: 0.8695 - val_acc: 0.6541\n",
      "Epoch 73/150\n",
      "2590/2590 [==============================] - 0s 61us/step - loss: 0.8183 - acc: 0.6737 - val_loss: 0.8614 - val_acc: 0.6505\n",
      "Epoch 74/150\n",
      "2590/2590 [==============================] - 0s 55us/step - loss: 0.8239 - acc: 0.6591 - val_loss: 0.8694 - val_acc: 0.6459\n",
      "Epoch 75/150\n",
      "2590/2590 [==============================] - 0s 67us/step - loss: 0.8163 - acc: 0.6668 - val_loss: 0.8688 - val_acc: 0.6559\n",
      "Epoch 76/150\n",
      "2590/2590 [==============================] - 0s 55us/step - loss: 0.8160 - acc: 0.6737 - val_loss: 0.8530 - val_acc: 0.6486\n",
      "Epoch 77/150\n",
      "2590/2590 [==============================] - 0s 55us/step - loss: 0.8172 - acc: 0.6718 - val_loss: 0.8525 - val_acc: 0.6658\n",
      "Epoch 78/150\n",
      "2590/2590 [==============================] - 0s 61us/step - loss: 0.8084 - acc: 0.6641 - val_loss: 0.8486 - val_acc: 0.6613\n",
      "Epoch 79/150\n",
      "2590/2590 [==============================] - 0s 75us/step - loss: 0.8068 - acc: 0.6734 - val_loss: 0.8507 - val_acc: 0.6505\n",
      "Epoch 80/150\n",
      "2590/2590 [==============================] - 0s 57us/step - loss: 0.8081 - acc: 0.6656 - val_loss: 0.8486 - val_acc: 0.6568\n",
      "Epoch 81/150\n",
      "2590/2590 [==============================] - 0s 56us/step - loss: 0.8129 - acc: 0.6668 - val_loss: 0.8450 - val_acc: 0.6577\n",
      "Epoch 82/150\n",
      "2590/2590 [==============================] - 0s 56us/step - loss: 0.8051 - acc: 0.6687 - val_loss: 0.8454 - val_acc: 0.6658\n",
      "Epoch 83/150\n",
      "2590/2590 [==============================] - 0s 52us/step - loss: 0.8111 - acc: 0.6668 - val_loss: 0.8518 - val_acc: 0.6640\n",
      "Epoch 84/150\n",
      "2590/2590 [==============================] - 0s 54us/step - loss: 0.8069 - acc: 0.6637 - val_loss: 0.8435 - val_acc: 0.6595\n",
      "Epoch 85/150\n",
      "2590/2590 [==============================] - 0s 56us/step - loss: 0.7997 - acc: 0.6722 - val_loss: 0.8374 - val_acc: 0.6532\n",
      "Epoch 86/150\n",
      "2590/2590 [==============================] - 0s 57us/step - loss: 0.7977 - acc: 0.6722 - val_loss: 0.8367 - val_acc: 0.6685\n",
      "Epoch 87/150\n",
      "2590/2590 [==============================] - 0s 59us/step - loss: 0.7949 - acc: 0.6683 - val_loss: 0.8335 - val_acc: 0.6730\n",
      "Epoch 88/150\n",
      "2590/2590 [==============================] - 0s 53us/step - loss: 0.7978 - acc: 0.6676 - val_loss: 0.8394 - val_acc: 0.6486\n",
      "Epoch 89/150\n",
      "2590/2590 [==============================] - 0s 57us/step - loss: 0.7949 - acc: 0.6660 - val_loss: 0.8449 - val_acc: 0.6775\n",
      "Epoch 90/150\n",
      "2590/2590 [==============================] - 0s 58us/step - loss: 0.7866 - acc: 0.6772 - val_loss: 0.8390 - val_acc: 0.6541\n",
      "Epoch 91/150\n",
      "2590/2590 [==============================] - 0s 59us/step - loss: 0.7820 - acc: 0.6792 - val_loss: 0.8315 - val_acc: 0.6649\n",
      "Epoch 92/150\n",
      "2590/2590 [==============================] - 0s 61us/step - loss: 0.7829 - acc: 0.6749 - val_loss: 0.8425 - val_acc: 0.6595\n",
      "Epoch 93/150\n",
      "2590/2590 [==============================] - 0s 61us/step - loss: 0.7817 - acc: 0.6811 - val_loss: 0.8316 - val_acc: 0.6739\n",
      "Epoch 94/150\n",
      "2590/2590 [==============================] - 0s 56us/step - loss: 0.7791 - acc: 0.6822 - val_loss: 0.8262 - val_acc: 0.6559\n",
      "Epoch 95/150\n",
      "2590/2590 [==============================] - 0s 53us/step - loss: 0.7759 - acc: 0.6807 - val_loss: 0.8462 - val_acc: 0.6640\n",
      "Epoch 96/150\n",
      "2590/2590 [==============================] - 0s 59us/step - loss: 0.7819 - acc: 0.6730 - val_loss: 0.8407 - val_acc: 0.6757\n",
      "Epoch 97/150\n",
      "2590/2590 [==============================] - 0s 51us/step - loss: 0.7928 - acc: 0.6672 - val_loss: 0.8277 - val_acc: 0.6703\n",
      "Epoch 98/150\n",
      "2590/2590 [==============================] - 0s 54us/step - loss: 0.7730 - acc: 0.6730 - val_loss: 0.8432 - val_acc: 0.6550\n",
      "\n",
      "Epoch 00098: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 99/150\n",
      "2590/2590 [==============================] - 0s 59us/step - loss: 0.7700 - acc: 0.6799 - val_loss: 0.8129 - val_acc: 0.6721\n",
      "Epoch 100/150\n",
      "2590/2590 [==============================] - 0s 54us/step - loss: 0.7635 - acc: 0.6846 - val_loss: 0.8133 - val_acc: 0.6712\n",
      "Epoch 101/150\n",
      "2590/2590 [==============================] - 0s 57us/step - loss: 0.7632 - acc: 0.6849 - val_loss: 0.8122 - val_acc: 0.6739\n",
      "Epoch 102/150\n",
      "2590/2590 [==============================] - 0s 60us/step - loss: 0.7636 - acc: 0.6795 - val_loss: 0.8129 - val_acc: 0.6730\n",
      "Epoch 103/150\n",
      "2590/2590 [==============================] - 0s 59us/step - loss: 0.7624 - acc: 0.6811 - val_loss: 0.8115 - val_acc: 0.6685\n",
      "Epoch 104/150\n",
      "2590/2590 [==============================] - 0s 62us/step - loss: 0.7626 - acc: 0.6799 - val_loss: 0.8114 - val_acc: 0.6694\n",
      "Epoch 105/150\n",
      "2590/2590 [==============================] - 0s 56us/step - loss: 0.7620 - acc: 0.6792 - val_loss: 0.8114 - val_acc: 0.6640\n",
      "Epoch 106/150\n",
      "2590/2590 [==============================] - 0s 54us/step - loss: 0.7626 - acc: 0.6822 - val_loss: 0.8112 - val_acc: 0.6703\n",
      "Epoch 107/150\n",
      "2590/2590 [==============================] - 0s 53us/step - loss: 0.7617 - acc: 0.6807 - val_loss: 0.8109 - val_acc: 0.6694\n",
      "Epoch 108/150\n",
      "2590/2590 [==============================] - 0s 55us/step - loss: 0.7615 - acc: 0.6795 - val_loss: 0.8124 - val_acc: 0.6721\n",
      "Epoch 109/150\n",
      "2590/2590 [==============================] - 0s 54us/step - loss: 0.7613 - acc: 0.6838 - val_loss: 0.8107 - val_acc: 0.6694\n",
      "Epoch 110/150\n",
      "2590/2590 [==============================] - 0s 53us/step - loss: 0.7627 - acc: 0.6799 - val_loss: 0.8105 - val_acc: 0.6676\n",
      "Epoch 111/150\n",
      "2590/2590 [==============================] - 0s 54us/step - loss: 0.7607 - acc: 0.6811 - val_loss: 0.8148 - val_acc: 0.6784\n",
      "Epoch 112/150\n",
      "2590/2590 [==============================] - 0s 54us/step - loss: 0.7625 - acc: 0.6803 - val_loss: 0.8106 - val_acc: 0.6694\n",
      "Epoch 113/150\n",
      "2590/2590 [==============================] - 0s 62us/step - loss: 0.7605 - acc: 0.6811 - val_loss: 0.8098 - val_acc: 0.6658\n",
      "Epoch 114/150\n",
      "2590/2590 [==============================] - 0s 59us/step - loss: 0.7605 - acc: 0.6830 - val_loss: 0.8104 - val_acc: 0.6712\n",
      "Epoch 115/150\n",
      "2590/2590 [==============================] - 0s 69us/step - loss: 0.7600 - acc: 0.6799 - val_loss: 0.8101 - val_acc: 0.6721\n",
      "Epoch 116/150\n",
      "2590/2590 [==============================] - 0s 61us/step - loss: 0.7608 - acc: 0.6799 - val_loss: 0.8113 - val_acc: 0.6703\n",
      "Epoch 117/150\n",
      "2590/2590 [==============================] - 0s 60us/step - loss: 0.7602 - acc: 0.6819 - val_loss: 0.8113 - val_acc: 0.6748\n",
      "\n",
      "Epoch 00117: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 118/150\n",
      "2590/2590 [==============================] - 0s 63us/step - loss: 0.7596 - acc: 0.6830 - val_loss: 0.8099 - val_acc: 0.6712\n",
      "Epoch 00118: early stopping\n",
      "1110/1110 [==============================] - 1s 583us/step\n"
     ]
    }
   ],
   "source": [
    "def create_model_1Dconv():\n",
    "    \"\"\"creates a start model to be completed by the NeuralTester class\"\"\"\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Conv1D(4, 7, strides=2, input_shape=(100, 1), activation=\"relu\"))\n",
    "    #model.add(BatchNormalization())\n",
    "    #model.add(LeakyReLU(alpha=0.3))\n",
    "    #model.add(Conv1D(16, 3, activation='relu'))\n",
    "    model.add(MaxPool1D(pool_size=2))\n",
    "    \n",
    "    model.add(Conv1D(4, 5, strides=1, activation=\"relu\"))\n",
    "    #model.add(BatchNormalization())\n",
    "    #model.add(LeakyReLU(alpha=0.3))\n",
    "    #model.add(Conv1D(16, 3, activation='relu'))\n",
    "    model.add(MaxPool1D(pool_size=2))\n",
    "    \n",
    "    #model.add(Conv1D(16, 5))\n",
    "    #model.add(BatchNormalization())\n",
    "    #model.add(LeakyReLU(alpha=0.3))\n",
    "    #model.add(Conv1D(16, 3, activation='relu'))\n",
    "    #model.add(MaxPool1D(pool_size=2))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    return model\n",
    "\n",
    "# Instantiate our testing class and fit regression & classification\n",
    "tester = NeuralTester(create_model_1Dconv, data_ns_aug, labels_ns_aug, epochs=150, batch_size=64)\n",
    "clss = tester.run_classification()\n",
    "\n",
    "# Visual inspection of results\n",
    "#visual_eval(data['y_regr']['test'], pred, cls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tune the hyperparameters of the neural net\n",
    "\n",
    "These parameters could be:\n",
    "-  filter size for conv nets (int)\n",
    "-  num. of filters for each conv layer (int)\n",
    "-  num. of units in the FC layer (int)\n",
    "-  dropout rate (float)\n",
    "-  etc.\n",
    "\n",
    "One could use either a grid search or a Baysean optimization process to find an optimal set of hyperparameters. In each case a new net is constructed and evaluated using CV on the validation set. The best net is then evaluated against a final test set which was set aside in the beginning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "307px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
