{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning of melting curves\n",
    "\n",
    "Here I use several ANN architectures and deep learning techniques to develop a model that can classify melting curves and compute the melting point from the shape of the curves  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# import all the modules/classes/functions\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Conv1D, MaxPool1D, Softmax, Dropout, Input, \\\n",
    "    Flatten, BatchNormalization, Reshape, LeakyReLU\n",
    "from keras import optimizers\n",
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "from keras.wrappers.scikit_learn import KerasClassifier, KerasRegressor\n",
    "from keras.utils import to_categorical\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define some useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     1,
     10,
     17,
     24
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import data and labels from stored archive and encode labels\n",
    "def import_data(filename):\n",
    "    \"\"\"imports the data/labels from a npz file and stores it into two np arrays\"\"\"\n",
    "    with np.load(filename) as npzfile:\n",
    "        data = npzfile[\"data\"]\n",
    "        targets = npzfile[\"targets\"]\n",
    "    \n",
    "    return data, targets\n",
    "\n",
    "# compute min/max values accross the entire training dataset\n",
    "def scale_train_dataset(data):\n",
    "    \"\"\"data is a numpy ndarray\"\"\"\n",
    "    min_val, max_val = np.min(data), np.max(data)\n",
    "    scaled = (data - min_val) / (max_val - min_val)\n",
    "    return (scaled, min_val, max_val)\n",
    "\n",
    "# apply the min/max values obtained from the training set on the test set\n",
    "def scale_test_dataset(data, min_val, max_val):\n",
    "    \"\"\"data - a numpy ndarray (test data)\n",
    "    min_val, max_val - floats: min/max values obtained from the training set\"\"\"\n",
    "    scaled = (data - min_val) / (max_val - min_val)\n",
    "    return scaled\n",
    "\n",
    "# reshape the targets array\n",
    "def reshape_targets(targets):\n",
    "    \"\"\"reshapes a 1D array into a 2D array\n",
    "    needed for tensorflow graphs/keras models\"\"\"\n",
    "    dim = targets.shape[0]\n",
    "    reshaped = targets.reshape((dim, 1))\n",
    "    return reshaped\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and shuffle the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     1
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load augmented non-scaled data\n",
    "data, targets = import_data(\"data_targets_aug.npz\")\n",
    "\n",
    "# shuffle the data/labels arrays consistently\n",
    "data, targets = shuffle(data, targets, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split into training and test sets\n",
    "\n",
    "Do this for both classification (prediction of curve labels) and regression (prediction of melting points). For regression, disgard the curves that belong to **class 0** (they contain too much noise and too little information to compute the melting points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split into classification and regression subsets\n",
    "# classification\n",
    "data_clf, targets_clf = data.copy(), targets[:,0].copy()\n",
    "\n",
    "# regression\n",
    "reg_mask = ~np.isnan(targets[:,1])\n",
    "data_reg, targets_reg = data[reg_mask], targets[:,1][reg_mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split data for performance evaluation (use stratified sampling due to class imbalances)\n",
    "data_clf_train, data_clf_test, labels_train, labels_test = train_test_split(data, targets_clf, test_size=0.2,\n",
    "                                                                   random_state=0, stratify=targets_clf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale the data\n",
    "\n",
    "For the GD algorithm, the curves need to be scaled to an interval of [0,1]. Since the features represent signal values at particular temperature points, I will use `min - max` scaling where the min/max values are obtained for the entire training data tensor (i.e. min/max values are for all feature values accross all curves in the training set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scale data for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.55718159,  0.55679027,  0.55621974, ...,  0.5632732 ,\n",
       "         0.5668126 ,  0.57142043],\n",
       "       [ 0.49458402,  0.49546699,  0.49654111, ...,  0.78559074,\n",
       "         0.78784806,  0.7903608 ],\n",
       "       [ 0.28658955,  0.29350023,  0.29823064, ...,  0.32790883,\n",
       "         0.32816828,  0.32909247],\n",
       "       ..., \n",
       "       [ 0.4850246 ,  0.48388165,  0.48328508, ...,  0.69854462,\n",
       "         0.70248938,  0.70637045],\n",
       "       [ 0.37028861,  0.37097712,  0.37109989, ...,  0.49555329,\n",
       "         0.50682777,  0.52254469],\n",
       "       [ 0.48248999,  0.50751835,  0.52853188, ...,  0.71060745,\n",
       "         0.69914026,  0.68284666]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# also obtain the min/max values to use on the test set\n",
    "data_clf_scaled, clf_min, clf_max = scale_train_dataset(data_clf_train)\n",
    "data_clf_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reshape data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels_train = reshape_targets(labels_train)\n",
    "labels_test = reshape_targets(labels_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plan\n",
    "\n",
    "### Types of ANNs to try:\n",
    "\n",
    "1. MLPs\n",
    "2. CNNs\n",
    "\n",
    "### CNN architectures/params to try:\n",
    "\n",
    "- 2x 1D convolutions (#filters=16, filter_size=3, activation=ReLU)\n",
    "- Batch normalization (?) (maybe I add it after each conv)\n",
    "- Max pooling after each conv step (size=2)\n",
    "- Flatten tensor prior to 1st FC layer\n",
    "- 1 FC hidden layer (32 units to start with, activation=ReLU)\n",
    "- 1 output layer (4 units, activation=Softmax)\n",
    "- If overfitting, add dropout layers! (p=0.5)\n",
    "\n",
    "### backprop params:\n",
    "\n",
    "- loss func: categorical cross-entropy\n",
    "- optm. algorithm: adam (or sgd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Neural Tester class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": [
     4,
     15,
     45,
     55,
     64
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A class for building and testing ANN models\n",
    "\n",
    "class NeuralTester(object):\n",
    "    \"\"\"class for testing neural net architectures\"\"\"\n",
    "    def __init__(self, base_build_fn, data, targets, epochs=50, batch_size=32):\n",
    "        \"\"\"base_build_fn - a function object that builds the basic network (returns a keras model)\n",
    "        data - rank 2 tensor of shape (n_samples, n_features)\n",
    "        targets - rank 2 tensor of shape (n_samples, 1)\"\"\"\n",
    "        self.base_build_fn = base_build_fn\n",
    "        self.data = data\n",
    "        self.targets = targets\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    \n",
    "    def fit_predict(self, build_fn, scikit_wrapper, target_name):\n",
    "        \"\"\"create scikit-estimator from a keras build function, fit to training, predict on test\"\"\"\n",
    "        print(\"\\nFitting the scikit estimator {}\\n{}\".format(scikit_wrapper.__name__, '='*40))\n",
    "        build_fn().summary() # create a keras model (build the graph)\n",
    "        # create the sklearn estimator (either KerasClassifier or KerasRegressor)\n",
    "        estimator = scikit_wrapper(\n",
    "            build_fn=build_fn, \n",
    "            epochs=self.epochs, \n",
    "            batch_size=self.batch_size, \n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        # split data to train/validation sets\n",
    "        data_train, data_test, targets_train, targets_test = train_test_split(self.data, self.targets,\n",
    "                                                                           test_size=0.3,\n",
    "                                                                           random_state=0,\n",
    "                                                                           stratify=self.targets)\n",
    "        \n",
    "        # fit the estimator using the training set\n",
    "        estimator.fit(data_train, targets_train, \n",
    "            validation_data=(data_test, targets_test), \n",
    "            callbacks=[\n",
    "                ReduceLROnPlateau(patience=3, verbose=1, factor=0.1),\n",
    "                EarlyStopping(patience=5, verbose=1)\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        # return predictions on test set\n",
    "        return estimator.predict(data_test)\n",
    "    \n",
    "    def create_build_fn(self, loss, activation, optimizer='adam', metrics=[]):\n",
    "        \"\"\"creates the final build function by adding the output layer,\n",
    "        output activitation function, loss function, and optimizer\"\"\"\n",
    "        def build_fn():\n",
    "            model = self.base_build_fn() # first build the base model\n",
    "            model.add(Dense(1, activation=activation)) # add output layer (2 classes)\n",
    "            model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
    "            return model\n",
    "        return build_fn\n",
    " \n",
    "    def run_regression(self):\n",
    "        \"\"\"use the build function to attempt a regression task\"\"\"\n",
    "        return self.fit_predict(\n",
    "            self.create_build_fn('mean_squared_error', 'linear'), \n",
    "            KerasRegressor, \n",
    "            'y_regr'\n",
    "        )\n",
    "        \n",
    "    \n",
    "    def run_classification(self):\n",
    "        \"\"\"use the build function to attempt a classification task\"\"\"\n",
    "        return self.fit_predict(\n",
    "            self.create_build_fn('binary_crossentropy', 'softmax', metrics=['accuracy']), \n",
    "            KerasClassifier, \n",
    "            'y_cls'\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and train a simple MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_model_FC():\n",
    "    \"\"\"creates a simple MLP\"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(Dense(16, input_dim=100, activation=\"relu\"))\n",
    "    model.add(Dense(16, activation=\"sigmoid\"))\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fitting the scikit estimator KerasClassifier\n",
      "========================================\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_13 (Dense)             (None, 16)                1616      \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 1,905\n",
      "Trainable params: 1,905\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 8400 samples, validate on 3600 samples\n",
      "Epoch 1/10\n",
      "8400/8400 [==============================] - 1s 62us/step - loss: 6.6047 - acc: 0.5857 - val_loss: 6.6072 - val_acc: 0.5856\n",
      "Epoch 2/10\n",
      "8400/8400 [==============================] - 0s 37us/step - loss: 6.6047 - acc: 0.5857 - val_loss: 6.6072 - val_acc: 0.5856\n",
      "Epoch 3/10\n",
      "8400/8400 [==============================] - 0s 32us/step - loss: 6.6047 - acc: 0.5857 - val_loss: 6.6072 - val_acc: 0.5856\n",
      "Epoch 4/10\n",
      "8400/8400 [==============================] - 0s 44us/step - loss: 6.6047 - acc: 0.5857 - val_loss: 6.6072 - val_acc: 0.5856\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 5/10\n",
      "8400/8400 [==============================] - 0s 33us/step - loss: 6.6047 - acc: 0.5857 - val_loss: 6.6072 - val_acc: 0.5856\n",
      "Epoch 6/10\n",
      "8400/8400 [==============================] - 0s 34us/step - loss: 6.6047 - acc: 0.5857 - val_loss: 6.6072 - val_acc: 0.5856\n",
      "Epoch 00006: early stopping\n",
      "3600/3600 [==============================] - 0s 25us/step\n"
     ]
    }
   ],
   "source": [
    "# instantiate the test class and train a MPL\n",
    "tester_fc = NeuralTester(create_model_FC, data_clf_train, labels_train, epochs=10, batch_size=100)\n",
    "clss_fc = tester_fc.run_classification()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and train a 1D CNN\n",
    "\n",
    "### Reshape data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": [
     1
    ],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12000, 100, 1)\n"
     ]
    }
   ],
   "source": [
    "# reshape data into a rank 3 tensor: (n_samples, 100, 1). Required by 1D conv layer\n",
    "data_clf_scaled_cnn = data_clf_scaled.reshape(data_clf_scaled.shape[0], data_clf_scaled.shape[1], 1)\n",
    "print(data_clf_scaled_cnn.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_model_1Dconv():\n",
    "    \"\"\"creates a start model to be completed by the NeuralTester class\"\"\"\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Conv1D(4, 7, strides=2, input_shape=(100, 1), activation=\"relu\"))\n",
    "    #model.add(BatchNormalization())\n",
    "    #model.add(LeakyReLU(alpha=0.3))\n",
    "    #model.add(Conv1D(16, 3, activation='relu'))\n",
    "    model.add(MaxPool1D(pool_size=2))\n",
    "    \n",
    "    model.add(Conv1D(4, 5, strides=1, activation=\"relu\"))\n",
    "    #model.add(BatchNormalization())\n",
    "    #model.add(LeakyReLU(alpha=0.3))\n",
    "    #model.add(Conv1D(16, 3, activation='relu'))\n",
    "    model.add(MaxPool1D(pool_size=2))\n",
    "    \n",
    "    #model.add(Conv1D(16, 5))\n",
    "    #model.add(BatchNormalization())\n",
    "    #model.add(LeakyReLU(alpha=0.3))\n",
    "    #model.add(Conv1D(16, 3, activation='relu'))\n",
    "    #model.add(MaxPool1D(pool_size=2))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a Conv. Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fitting the scikit estimator KerasClassifier\n",
      "========================================\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 47, 4)             32        \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 23, 4)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 19, 4)             84        \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 9, 4)              0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 36)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                2368      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 2,549\n",
      "Trainable params: 2,549\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 8400 samples, validate on 3600 samples\n",
      "Epoch 1/10\n"
     ]
    }
   ],
   "source": [
    "# Instantiate our testing class and train\n",
    "tester = NeuralTester(create_model_1Dconv,\n",
    "                      data_clf_scaled_cnn,\n",
    "                      labels_train,\n",
    "                      epochs=10,\n",
    "                      batch_size=100)\n",
    "\n",
    "clss = tester.run_classification()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tune the hyperparameters of the neural net\n",
    "\n",
    "These parameters could be:\n",
    "-  filter size for conv nets (int)\n",
    "-  num. of filters for each conv layer (int)\n",
    "-  num. of units in the FC layer (int)\n",
    "-  dropout rate (float)\n",
    "-  etc.\n",
    "\n",
    "One could use either a grid search or a Baysean optimization process to find an optimal set of hyperparameters. In each case a new net is constructed and evaluated using CV on the validation set. The best net is then evaluated against a final test set which was set aside in the beginning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "307px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
